#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EmbeddingService - Service d'embeddings pour DARYL Sharding Memory
Utilise sentence-transformers pour g√©n√©rer des embeddings s√©mantiques
"""

import json
import hashlib
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Union

# Optional: Import sentence-transformers
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

class DummyModel:
    """Mod√®le factice pour les tests (√©vite le t√©l√©chargement)"""
    
    def __init__(self, model_name="all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.dimension = 384  # Taille standard pour all-MiniLM-L6-v2
    
    def encode(self, texts, convert_to_numpy=True, normalize_embeddings=False, **kwargs):
        """
        G√©n√®re des embeddings d√©terministes pour √©viter les downloads
        
        Args:
            texts: Texte ou liste de textes √† encoder
            convert_to_numpy: Retourner numpy array (True)
            normalize_embeddings: Normaliser les embeddings (False)
            **kwargs: Arguments suppl√©mentaires ignor√©s
            
        Returns:
            Embeddings numpy array (shape: [n, 384])
        """
        # Convertir en liste si n√©cessaire
        if isinstance(texts, str):
            texts = [texts]
        
        # G√©n√©rer des embeddings d√©terministes bas√©s sur le hash du texte
        embeddings = []
        for text in texts:
            # Hash du texte pour g√©n√©ration d√©terministe
            s = sum(ord(c) for c in text.strip().lower()) % 1000
            s_norm = (s + 1) / 1001.0
            
            # Cr√©er un embedding pseudo-al√©atoire mais d√©terministe
            arr = []
            for i in range(384):
                # Seed bas√© sur hash du texte + index
                np.random.seed(s + i * 1000)
                val = (np.random.rand() - 0.5) * 2.0  # Valeur entre -1 et 1
                arr.append(val)
            
            embeddings.append(arr)
        
        return np.array(embeddings, dtype=np.float32)


class EmbeddingService:
    """Service pour g√©n√©rer et mettre en cache des embeddings"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2", model: Optional[Union[str, DummyModel]] = None):
        """
        Initialise le service d'embeddings
        
        Args:
            model_name: Nom du mod√®le sentence-transformers
            model: Mod√®le optionnel (pour tests/mocks)
        """
        self.model_name = model_name
        self.model = model  # Permet d'injecter un mod√®le (ex: DummyModel pour tests)
        self.cache = {}  # Cache en m√©moire pour les embeddings
        self._real_model = None  # Mod√®le r√©el (lazy load)
        self._dimension = 384  # Taille par d√©faut
        
        # Ne PAS charger le mod√®le dans __init__ (Lazy Load)
        print(f"‚úÖ EmbeddingService initialis√© (model_name: {model_name})")
    
    def _get_model(self):
        """
        Charge le mod√®le r√©el (Lazy Load) au premier appel
        
        Returns:
            Mod√®le (SentenceTransformer ou DummyModel)
        """
        # Si un mod√®le inject√© (ex: DummyModel), l'utiliser
        if self.model is not None:
            return self.model
        
        # Si mod√®le r√©el d√©j√† charg√©, le retourner
        if self._real_model is not None:
            return self._real_model
        
        # Sinon, charger le mod√®le r√©el
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            print("‚ö†Ô∏è sentence-transformers non disponible. Utilisation DummyModel.")
            self.model = DummyModel(self.model_name)
            self._real_model = self.model
            self._dimension = self.model.dimension
            return self.model
        
        try:
            print(f"üì• Chargement du mod√®le r√©el: {self.model_name}")
            self._real_model = SentenceTransformer(self.model_name)
            self._dimension = self._real_model.get_sentence_embedding_dimension()
            print(f"‚úÖ Mod√®le r√©el charg√©: {self.model_name} (dimension: {self._dimension})")
            return self._real_model
        except Exception as e:
            print(f"‚ùå Erreur chargement mod√®le r√©el: {e}")
            print("‚ö†Ô∏è Utilisation DummyModel en cas d'√©chec.")
            self.model = DummyModel(self.model_name)
            self._real_model = self.model
            self._dimension = self.model.dimension
            return self.model
    
    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """
        G√©n√®re un embedding pour un texte
        
        Args:
            text: Texte √† encoder
            
        Returns:
            Liste de floats (embedding vector) ou None si erreur
        """
        # V√©rifier le cache
        text_hash = self._hash_text(text)
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        try:
            # Obtenir le mod√®le (Lazy Load)
            model = self._get_model()
            
            # G√©n√©rer l'embedding
            embedding = model.encode(text, convert_to_numpy=False)
            
            # Si c'est un tensor, le convertir en liste
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()
            elif isinstance(embedding, np.ndarray):
                embedding = embedding.tolist()
            
            # Mettre en cache
            self.cache[text_hash] = embedding
            
            return embedding
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration embedding: {e}")
            return None
    
    def batch_generate_embeddings(self, texts: List[str]) -> Dict[str, Optional[List[float]]]:
        """
        G√©n√®re des embeddings pour plusieurs textes (batch)
        
        Args:
            texts: Liste de textes √† encoder
            
        Returns:
            Dictionnaire {text_hash: embedding} ou {} si erreur
        """
        results = {}
        
        try:
            # Obtenir le mod√®le (Lazy Load)
            model = self._get_model()
            
            # G√©n√©rer en batch pour optimiser
            embeddings = model.encode(texts, convert_to_numpy=False)
            
            # Si c'est un tensor, le convertir en liste de listes
            if hasattr(embeddings, 'tolist'):
                embeddings = embeddings.tolist()
            elif isinstance(embeddings, np.ndarray):
                embeddings = embeddings.tolist()
            
            # Si c'est une liste unique, la mettre dans une liste
            if isinstance(embeddings, list) and len(embeddings) > 0 and not isinstance(embeddings[0], list):
                embeddings = [embeddings]
            
            # Mettre en cache
            for text, embedding in zip(texts, embeddings):
                text_hash = self._hash_text(text)
                results[text_hash] = embedding
            
            return results
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration batch: {e}")
            return {}
    
    def _hash_text(self, text: str) -> str:
        """
        G√©n√®re un hash unique pour le cache
        
        Args:
            text: Texte √† hasher
            
        Returns:
            Hash SHA256 du texte
        """
        # Normaliser le texte pour √©viter les probl√®mes d'encodage
        normalized = text.strip().lower()
        return hashlib.sha256(normalized.encode('utf-8')).hexdigest()
    
    def get_cache_stats(self) -> Dict[str, int]:
        """
        Retourne les statistiques du cache
        
        Returns:
            Dictionnaire avec cache_size, cache_hits
        """
        return {
            "cache_size": len(self.cache),
            "model_name": self.model_name,
            "model_type": "DummyModel" if isinstance(self.model, DummyModel) else "SentenceTransformer",
            "embedding_dimension": self._dimension
        }
    
    def clear_cache(self):
        """Vide le cache d'embeddings"""
        self.cache.clear()
        print("üóëÔ∏è Cache d'embeddings vid√©")
    
    def save_cache_to_file(self, file_path: str):
        """
        Sauvegarde le cache dans un fichier JSON
        
        Args:
            file_path: Chemin du fichier de sauvegarde
        """
        try:
            # Convertir les listes numpy en listes Python standard
            cache_serializable = {
                k: v.tolist() if hasattr(v, 'tolist') else v 
                for k, v in self.cache.items()
            }
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(cache_serializable, f, indent=2, ensure_ascii=False)
            
            print(f"‚úÖ Cache sauvegard√© dans {file_path}")
        except Exception as e:
            print(f"‚ùå Erreur sauvegarde cache: {e}")
    
    def load_cache_from_file(self, file_path: str):
        """
        Charge le cache depuis un fichier JSON
        
        Args:
            file_path: Chemin du fichier de cache
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # Restaurer les embeddings
            self.cache = cache_data
            
            print(f"‚úÖ Cache charg√© depuis {file_path} ({len(cache_data)} embeddings)")
        except Exception as e:
            print(f"‚ùå Erreur chargement cache: {e}")


if __name__ == "__main__":
    # Test du service d'embeddings
    print("üß™ Test du service d'embeddings")
    print("   Mode: DUMMY MODEL (pas de download)")
    
    service = EmbeddingService(model_name="all-MiniLM-L6-v2")
    
    # Exemple d'utilisation
    test_text = "DARYL Sharding Memory est un syst√®me s√©mantique pour agents stateless"
    
    print(f"   Texte: {test_text}")
    
    embedding = service.generate_embedding(test_text)
    
    if embedding is not None:
        print(f"   Embedding dimension: {len(embedding)}")
        print(f"   Premier 5 valeurs: {embedding[:5]}")
        
        # Test de similarit√©
        test_text2 = "Les agents ont besoin de m√©moire persistante"
        embedding2 = service.generate_embedding(test_text2)
        
        if embedding2 is not None:
            # Similarit√© cosinus simple
            dot = sum(a * b for a, b in zip(embedding, embedding2))
            norm1 = sum(a * a for a in embedding)
            norm2 = sum(b * b for b in embedding2)
            similarity = dot / (norm1 * norm2) ** 0.5
            
            print(f"\nüß™ Test similarit√©:")
            print(f"   Texte 1: {test_text[:50]}...")
            print(f"   Texte 2: {test_text2[:50]}...")
            print(f"   Similarit√© cosinus: {similarity:.4f}")
    
    print(f"\nüìä Statistiques du cache:")
    stats = service.get_cache_stats()
    for k, v in stats.items():
        print(f"   {k}: {v}")
